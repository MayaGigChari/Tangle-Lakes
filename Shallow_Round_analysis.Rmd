---
title: "Tangle Lakes Analysis"
output: html_document
---

```{r setup, include=FALSE}

install.packages("recapr", repos = "https://cloud.r-project.org")
install.packages("sf", repos = "https://cloud.r-project.org")
install.packages("fishmethods", repos = "https://cloud.r-project.org")

library(knitr)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

#source the parent files that we wnat to work with. 
#need to already have all the packages installed or else these sourcings fail. 

#recapr, lubridate, plotly, sf
suppressWarnings({
source('Data_preparation_forRmd.R')
source('population_estimation.R') #this is almost entireley functions. 
#we could probably get away from sourcing these again, but it's nice to generate and save all the images. 
source('growth_correction.R')
source('maps_and_viz.R')
source('Length_distribution.R')
})

#global variables 



```

## 1.0.0 First We will Look at the Shallow-Round Tangle Lake System. (Part 1)

Here Let's take an initial look at the cleaned dataframe, and parse the large dataset to only the shallow-round data
```{r, echo = FALSE}

summary(total_data)


#adding in the two floys that moved from lower to shallow-round. This can be reversed if we do not want to do it this way. 
shallow_round_lake_data<- total_data%>%
  filter(lake_combined == "Shallow_Round" | Floy %in% c("19970", "18608"))

shallow_round_lake_data$lake_combined = "Shallow_Round"

shallow_round_lake_data_floys<-shallow_round_lake_data%>%
  select(Floy)%>%
  filter(!is.na(Floy))
  
floys_shallow_round<-df_long%>%
  filter(Floy %in% shallow_round_lake_data_floys$Floy)

```



### 1.1 Evaluating Growth and Length Composition Analysis 

#### 1.1.1 Visualization of ECDF with and Without Length-correction for 2024 (N2) Fish


```{r, echo = FALSE}
#read in the full data


data_full<- read.csv("Total_data_corrected_lengths.csv")

#declare some variables

events_list<- c("n1", "n2", "m2")

min_size_capture_shallowround<- shallow_round_lake_data%>%
  filter(Date < "2024-01-01")%>%
  filter(!is.na(Fork_length))%>%
  summarize(
    min = min(as.numeric(Fork_length))
  )


#filter data

data_full_shallowround<- data_full %>%
  filter(lake_combined == "Shallow_Round")%>%
  filter(cap_label %in% events_list)

print("glimpse of filtered data for shallow-round")
head(data_full_shallowround)


#first let's look at the ecdf without correcting n2 lengths and without truncation of the minimum length 
ecdf_plot_withcorrection_shallowround_full_regression<-ggplot(data_full_shallowround, aes(x = as.numeric(Fork_length), color = grouping)) +
  stat_ecdf(geom = "step") +  # ECDF with step lines
  labs(x = "length", y = "CDF", color = "Group", title = "ECDF Shallow-Round using original lengths for both years and no min-size truncation") +
  theme_minimal()+
  theme(plot.title = element_text(size = 10))

#show the plot
ecdf_plot_withcorrection_shallowround_full_regression

#save it 
ggsave("ecdf_plot_withcorrection_shallowround_full_regression.png", ecdf_plot_withcorrection_shallowround_full_regression, dpi = 300)

#Now let's truncate to the minimum captured size in event 1, which as we said above is min_size_capture_shallowround


#we keep fewer allowable fish using the full regression than just the shallow-round regression
data_full_shallowround_truncated<- data_full_shallowround%>%
  filter(corrected_length>= min_size_capture_shallowround$min)


ecdf_plot_withcorrection_shallowround_full_regression_truncated<-ggplot(data_full_shallowround_truncated, aes(x = as.numeric(corrected_length), color = grouping)) +
  stat_ecdf(geom = "step") +  # ECDF with step lines
  labs(x = "length", y = "CDF", color = "Group", title = "n2-length corrected ECDF Shallow-Round, with truncation at min 2023 size") +
  theme_minimal()+
  theme(plot.title = element_text(size = 10))

#show the plot
ecdf_plot_withcorrection_shallowround_full_regression_truncated

#save it 
ggsave("ecdf_plot_withcorrection_shallowround_full_regression_truncated.png", ecdf_plot_withcorrection_shallowround_full_regression_truncated, dpi = 300)
```
As we can see above, in the first plot we observe the cumulitive distribution function of length in 2023 (n1) and 2024 (n2) that is not corrected for growth. In the second plot, we observe the cdf of length in 2023 (n1) and 2024 (n2*), in which 2024 lengths are corrected to what they "would have been" in 2023. This correction assumes that the population is NOT at some kind of equilibrium, and therefore shifts the entire n2 distribution to the left as seen above. 

#### 1.1.2 K-S tests with and without length-correction



```{r, echo=FALSE}

shallow_round_ks_nocorrection<- ks_test(data_full_shallowround, events_list, "Shallow-Round", length_variable = "Fork_length" )

shallow_round_ks_withcorrection<- ks_test(data_full_shallowround_truncated, events_list, "Shallow-Round", length_variable = "corrected_length")

print("KS-test results without length correction:")
shallow_round_ks_nocorrection

print("KS test results with length correction: ")
shallow_round_ks_withcorrection

```

As we can see, the KS-test cites that the m (n1) and c(n2) events come from different distributions when the n2 event is corrected for growth, but not when the n2 event is not corrected for growth. 


Ultimately, without correcting for growth, results from the KS test indicate no stratification is required by length. 

#### 1.1.3 Length Composition Analysis Without growth correction 

Below we will assume that the population is in equilibrium, as is suggested by the similar ECDF of fish caught in round/shallow tangle-lake in 2023 and 2024. We will move forward with an estimate of length composition analysis under equilibrium pop dynamics. 

```{r, echo = FALSE}
#when making these distributions, we want to make sure that we are only considering n1 and n2, and not using the m2 data twice. 

data_full_shallowround_n1n2<- data_full_shallowround%>%
  filter(grouping %in% c("n1", "n2"))

data_full_shallowround_n1n2 <- data_full_shallowround_n1n2 %>%
  mutate(Year_capped = year(Date))

#now we can run some of the functions from the Length_distribution file. 


#just visualizing the continuous distribution first 

shallow_round_continuous_viz<- plot_length_dist_continuous(data_full_shallowround_n1n2, lakeSystem = "Shallow-Round")



print("Let's visualize the non-binned continuous length distribution in a few ways")

shallow_round_continuous_viz$full_distribution
shallow_round_continuous_viz$full_distribution_bothyears
shallow_round_continuous_viz$full_distribution_years_sep


#now we can do some statistics. 

#bin the lengths
data_full_shallowround_n1n2_binned<- assign_bins(data_full_shallowround_n1n2)

#do an analysis for both years, 2023 and 2024

shallowround_length_stats_bothyears<- binned_length_statistics(data_full_shallowround_n1n2_binned)

print("Now we have binned the shallow-round data, let's take a look at the stats for both years")
knitr::kable(shallowround_length_stats_bothyears, caption = "Binned Length Stats Shallow-Round 2023 + 2024")
#the preservation needs to happen here. 
shallowround_length_stats_2023<- binned_length_statistics(data_full_shallowround_n1n2_binned, yearSort = TRUE, year = 2023)


print("Now let's look at the length-distribution binned for 2023")
knitr::kable(shallowround_length_stats_2023, caption = "Binned Length Stats Shallow-Round 2023")

shallowround_length_stats_2024<- binned_length_statistics(data_full_shallowround_n1n2_binned, yearSort = TRUE, year = 2024)

print("Now let's look at the length-distribution binned for 2024")
knitr::kable(shallowround_length_stats_2024, caption = "Binned Length Stats Shallow-Round 2024")



print("Let's visualize the binned length distributions")


shallowround_length_stats_bothyears_plot<- plot_length_binnedWithError(shallowround_length_stats_bothyears)


shallowround_length_stats_2023_plot<- plot_length_binnedWithError(shallowround_length_stats_2023)

shallowround_length_stats_2024_plot<- plot_length_binnedWithError(shallowround_length_stats_2024)

print("Pooled Length Distribution 2023 and 2024")
shallowround_length_stats_bothyears_plot

print("Length Distribution 2023 only")
shallowround_length_stats_2023_plot

print("Length Distribution 2024 only")

shallowround_length_stats_2024_plot
```

#### 1.1.4 Length-By-Weight Analysis

Let's take a look at the length-by weight analysis using the LBV model 
```{r, echo = FALSE}

#fit the LBV length-by weight relationship allometric model. 

#aL^b is the model, we estimate parameters for a and b. 

#this is the allometric weight-length model 


wl.lin <- lm_lw(data_full_shallowround_n1n2)


summary(wl.lin)

# Remember, as a linear model the intercept is equal to ln(alpha) and the slope coefficient is equal to the beta parameter

#coef(wl.lin)

# Finally lets save our parameter estimates
# Remember if we want the mean-unbiased estimate of the alpha parameter we need
#   to account for the lognormal correction when re-transforming

wl.lin.sigma <- sigma(wl.lin) 

print("Sigma:")
wl.lin.sigma
# There is an analytical solution for the error standard deviation 

# ln(alpha)
wl.lin.ln_alpha <- as.numeric(coef(wl.lin)[1])

print("alpha:")
wl.lin.ln_alpha
# alpha (mean-unbiased)# this is from the fish 622 course. 
wl.lin.alpha <- exp(wl.lin.ln_alpha)*exp((wl.lin.sigma^2)/2)

# Beta
wl.lin.beta <- as.numeric(coef(wl.lin)[2])
print("beta:")
wl.lin.beta

# Lets plot the model fit
plot_wl(data=data_full_shallowround_n1n2, alpha=wl.lin.alpha, beta=wl.lin.beta)

#this is probably good information for some bayesian model.

#we can see, interestingly that the beta of about 3.4 is very similar to the beta value in the Fielding lake 2024 survey. 

#length proportions are in 25 mm fl categories 


#now we have done the length-weight relationship and the length composition analysis. 

#For fun, maybe we can see what these statistics and graphs would look like if we used the length-corrected data. 

```

Here we can see that our estimation for beta is about 3.4, which is similar to the isometric model and also similar to the beta value estimated in the Fielding lake 2024 survey. 



#### 1.1.5 What if we used the length-corrected data? Length composition analysis with length-corrected data 


```{r, echo = FALSE}

###Finally, let's do this one more time, but with length-corrected data that IS cutoff from 2024 to the minimum size captured in 2023. 
#this truncated data has corrected lengths for 2024 corrected to the minimum size for 2023. 


data_full_shallowround_trunc_copy<- data_full_shallowround_truncated

#do the assignment 
data_full_shallowround_trunc_copy$Fork_length<- data_full_shallowround_trunc_copy$corrected_length

#now we can proceed with the same analyses as above. 

#first we will do some data wrangling and visualization 

data_full_shallowround_n1n2_corrected_trunc<- data_full_shallowround_trunc_copy%>%
  filter(grouping %in% c("n1", "n2"))

data_full_shallowround_n1n2_corrected_trunc<- data_full_shallowround_n1n2_corrected_trunc %>%
  mutate(Year_capped = year(Date))

shallow_round_continuous_viz_trunc<- plot_length_dist_continuous(data_full_shallowround_n1n2_corrected_trunc, lakeSystem = "Shallow-Round")

#it only makes sense to look at both years now because the 2024 data has been corrected, so no longer really represents 2024.

print("Let's visualize the non-binned continuous length distribution in a few ways")


print("if we look at both years overlayed, we can see that with the correction the 2024 histogram has a maximum density at a much lower fork length when we correct 2024 lengths for growth") 


shallow_round_continuous_viz_trunc$full_distribution

shallow_round_continuous_viz_trunc$full_distribution_bothyears

#let's look at the total combined distribution 
shallow_round_continuous_viz_trunc$full_distribution


#now let's look at the years in separate panels. observe how shifted the data has become. 

#we really mess up the 2024 distribution when we truncate it to fit 2023. (really for this it would be an "extension" of the 2023 data. )
shallow_round_continuous_viz_trunc$full_distribution_years_sep

#bin the lengths
data_full_shallowround_n1n2_trunc_binned<- assign_bins(data_full_shallowround_n1n2_corrected_trunc)


shallowround_length_stats_bothyears_trunc<- binned_length_statistics(data_full_shallowround_n1n2_trunc_binned)


shallowround_length_stats_2023_corrected_trunc<- binned_length_statistics(data_full_shallowround_n1n2_trunc_binned, yearSort = TRUE, year = 2023)

shallowround_length_stats_2024_corrected_trunc<- binned_length_statistics(data_full_shallowround_n1n2_trunc_binned, yearSort = TRUE, year = 2024)


print("Let's visualize the binned length distributions")

shallowround_length_stats_bothyears_plot_corrected_trunc<- plot_length_binnedWithError(shallowround_length_stats_bothyears_trunc)

shallowround_length_stats_2023_plot_corrected_trunc<- plot_length_binnedWithError(shallowround_length_stats_2023_corrected_trunc)

shallowround_length_stats_2024_plot_corrected_trunc<- plot_length_binnedWithError(shallowround_length_stats_2024_corrected_trunc)



print("Pooled Length Distribution 2023 and 2024")
shallowround_length_stats_bothyears_plot_corrected_trunc

print("Length Distribution 2023 only")
shallowround_length_stats_2023_plot_corrected_trunc

print("Length Distribution 2024 only")

shallowround_length_stats_2024_plot_corrected_trunc

#as we can see, truncating the distribution is not necessarily the move! 

```
When we truncate the distribution, we get a right skew in our length-composition that does not reflect the true catchability. 


## 1.2 Estimating Population Size

#### 1.2.1 Consistency Tests for the Peterson Estimator

```{r, echo = FALSE}

#we use the geographic dataset generated in maps and viz. 
sf_df_nounk <- sf_df %>%
  filter(Floy != "UNK" | is.na(Floy))


#need to go by stats now. 
sf_df_shallowround_r1<- st_crop(sf_df_nounk, shallowround_lake_region_1)
sf_df_shallowround_r1$region<-1
sf_df_shallowround_r2<- st_crop(sf_df_nounk, shallowround_lake_region_2)
sf_df_shallowround_r2$region<-2
sf_df_shallowround_r3<- st_crop(sf_df_nounk, shallowround_lake_region_3)
sf_df_shallowround_r3$region<-3

sf_df_shallowround_regions<- sf_df_shallowround_r1%>%
  rbind(sf_df_shallowround_r2)%>%
  rbind(sf_df_shallowround_r3)
#total of 255. double check against this 
sf_df_nounk_shallowround<- sf_df_nounk%>%
  filter(Lake %in% c("Shallow", "Round"))%>%
  count()


#not sure what the correct format is for the chi2 test here. 
#review Logan's code. 
#looks like Logan did not have totals. 


#########################################################
###MATT TYERS' TESTS. mine doesn't work so we will use his for now.
#########################################################

### TEST FOR EQUAL PROBABILITY OF CAPTURE IN EVENT 1 AND EVENT 2

#first, let's remove the fish that were too small in event 1 to be caught 

sf_df_shallowround_regions_small_rem<-tibble(sf_df_shallowround_regions%>%filter(corrected_length >= min_size_capture_shallowround$min))
counts_by_strata_shallowround <-sf_df_shallowround_regions_small_rem %>% group_by(region) %>% 
  summarize(n1 = sum(stat == "n1"), n2 = sum(stat == "n2")+sum(stat == "m2"), m2 = sum(stat == "m2"))

matrix_shallowround<- petersen_matrix_tyerscode(sf_df_shallowround_regions_small_rem)
matrix_shallowround$"NA"<-NULL

consistencytest(counts_by_strata_shallowround$n1, counts_by_strata_shallowround$n2, stratamat = matrix_shallowround)


```
As we can see above, at least one of the null hypotheses was not rejected, so we can assume consistency of the peterson estimator. 


#### 1.2.2 Population Estimation

We will do a population estimation. We use the truncated dataset to correct for 2024 fish that were possible to catch in 2023. We will perform this estimate both including the two fish that moved from lower to shallow, and not including them. 

```{r, echo = FALSE}

####Below is the final pop estimate with the corrected length dataset. We use the truncated dataset to correct for 2024 fish that were possible to catch in 2023. 





#we apply the filters. 

data_full_shallowround_truncated_pop<- data_full_shallowround_truncated%>%
  filter(stat == grouping)%>%
  filter(cap_label %in% c("n1", "n2", "m2"))


shallow_round_lake_data_nomovers<- data_full_shallowround_truncated_pop%>%
  filter(!Floy %in% c("19970", "18608"))

#now we do the pop estimates. 

petersen_estimate_trunc<- pop_calc(petersen_funcs,  data_full_shallowround_truncated_pop)
Chapman_estimate_trunc<- pop_calc(Chapman_funcs, data_full_shallowround_truncated_pop)
bailey_estimate_trunc<- pop_calc(Bailey_funcs, data_full_shallowround_truncated_pop)


print("Petersen estimate for population size of Shallow-round lake with the 2 fish that moved")
petersen_estimate_trunc
petersen_estimate_nomovement<- pop_calc(petersen_funcs,  shallow_round_lake_data_nomovers)
Chapman_estimate_nomovement<- pop_calc(Chapman_funcs, shallow_round_lake_data_nomovers)
bailey_estimate_nomovement<- pop_calc(Bailey_funcs, shallow_round_lake_data_nomovers)

print("Petersen estimate for population size of Shallow-round lake without the 2 fish that moved")
petersen_estimate_nomovement

#as we can see, population estimates are significantly lower when we remove "uncatchable" n2 fish from the population. 

#We collected an appropriate amount of data to satisfy the objective precision criteria. 

#great! now we have a cool pipeline for everything. For the rest of the lakes, we will not do ALL these steps, we will use the simplifications where we can. 

```

Because we have such a small number of recaps in round/shallow, two fish makes a big difference. 

# 2.0.0 Now We will Look at the Lower Tangle Lake System. (Part 2)

We already determined in the growth_correction file that we can use pooled data to represent growth, and that there is not significant growth within years (there is significant growth within years, but only if we only remove negative outliers and there are very small sample sizes in this case.) 

Below I will use the corrected lengths for N2 using the pooled regressions. 


### 2.1 Evaluating Growth and Length Composition Analysis 

#### 2.1.1 Visualization of ECDF with and Without Length-correction for 2024 (N2) Fish

```{r, echo = FALSE}
data_full_Lower<- data_full %>%
  filter(lake_combined == "Lower")%>%
  filter(cap_label %in% events_list)

print("first let's look at the ecdf without truncating the distribution or correcting for length-changes") 
ecdf_plot_withcorrection_Lower_full_regression<-ggplot(data_full_Lower, aes(x = as.numeric(Fork_length), color = grouping)) +
  stat_ecdf(geom = "step") +  # ECDF with step lines
  labs(x = "length", y = "CDF", color = "Group", title = "m2-length corrected ECDF Lower using full regression, no min-truncation") +
  theme_minimal()

ecdf_plot_withcorrection_Lower_full_regression

ggsave("ecdf_plot_withcorrection_Lower_full_regression.png", ecdf_plot_withcorrection_shallowround_full_regression, dpi = 300)

print("Now let's truncate to the minimum captured size in event 1, and with corrected lengths for n2")

min_size_capture_Lower<- data_full_Lower%>%
  filter(Date < "2024-01-01")%>%
  filter(!is.na(Fork_length))%>%
  summarize(
    min = min(as.numeric(Fork_length))
  )


#we keep fewer allowable fish using the full regression than just the shallow-round regression
data_full_Lower_truncated<- data_full_Lower%>%
  filter(corrected_length>= min_size_capture_Lower$min)


ecdf_plot_withcorrection_Lower_full_regression_truncated<-ggplot(data_full_Lower_truncated, aes(x = as.numeric(corrected_length), color = grouping)) +
  stat_ecdf(geom = "step") +  # ECDF with step lines
  labs(x = "length", y = "CDF", color = "Group", title = "m2-length corrected ECDF Shallow-Round using full regression, with min-truncation") +
  theme_minimal()

ecdf_plot_withcorrection_Lower_full_regression_truncated

ggsave("ecdf_plot_withcorrection_Lower_full_regression_truncated.png", ecdf_plot_withcorrection_Lower_full_regression_truncated, dpi = 300)



```

#### 2.1.2 K-S tests with and without length-correction

```{r, echo = FALSE}

events_list = c("n1", "n2", "m2")

Lower_ks_nocorrection<- ks_test(data_full_Lower, events_list, "Lower", length_variable = "Fork_length" )

Lower_ks_withcorrection<- ks_test(data_full_Lower_truncated, events_list, "Lower", length_variable = "corrected_length")

print("KS-test results without length correction:")
Lower_ks_nocorrection

print("KS test results with length correction: ")
Lower_ks_withcorrection

```
Here, no matter whether we decide to truncate the data and correct for length changes or not, we see no evidence for the need to stratify by length. or to exclude one event.Therefore, we will continue with the analyses with data from both events. 

#### 2.1.3 Length Composition Analysis Without growth correction 


```{r, echo = FALSE}

#when making these distributions, we want to make sure that we are only considering n1 and n2, and not using the m2 data twice. 

data_full_Lower_n1n2<- data_full_Lower%>%
  filter(grouping %in% c("n1", "n2"))

data_full_Lower_n1n2 <- data_full_Lower_n1n2 %>%
  mutate(Year_capped = year(Date))

#now we can run some of the functions from the Length_distribution file. 


#just visualizing the continuous distribution first 

Lower_continuous_viz<- plot_length_dist_continuous(data_full_Lower_n1n2, lakeSystem = "Lower")


print("Let's visualize the non-binned continuous length distribution in a few ways")

Lower_continuous_viz$full_distribution
Lower_continuous_viz$full_distribution_bothyears
Lower_continuous_viz$full_distribution_years_sep

#bin the lengths

data_full_Lower_n1n2<- data_full_Lower_n1n2%>%
  drop_na(Fork_length)%>%
  filter(Fork_length!= "?")

data_full_Lower_n1n2_binned<- assign_bins(data_full_Lower_n1n2)

#do an analysis for both years, 2023 and 2024

Lower_length_stats_bothyears<- binned_length_statistics(data_full_Lower_n1n2_binned)


print("Now we have binned the lower lake data, let's take a look at the stats for both years")
knitr::kable(Lower_length_stats_bothyears, caption = "Binned Length Stats Lower 2023 + 2024")

#the preservation needs to happen here. 
Lower_length_stats_2023<- binned_length_statistics(data_full_Lower_n1n2_binned, yearSort = TRUE, year = 2023)

print("Let's look at just 2023 for lower lake")
knitr::kable(Lower_length_stats_2023, caption = "Binned Length Stats Lower 2023")

Lower_length_stats_2024<- binned_length_statistics(data_full_Lower_n1n2_binned, yearSort = TRUE, year = 2024)

print("Let's look at just 2024 for lower lake")
knitr::kable(Lower_length_stats_2024, caption = "Binned Length Stats Lower 2024")

#make some plots for both years, 2023 and 2024

Lower_length_stats_bothyears_plot<- plot_length_binnedWithError(Lower_length_stats_bothyears)


Lower_length_stats_2023_plot<- plot_length_binnedWithError(Lower_length_stats_2023)

Lower_length_stats_2024_plot<- plot_length_binnedWithError(Lower_length_stats_2024)

print("Binned Length Distribution for 2023/2024 combined ")
Lower_length_stats_bothyears_plot

print("Binned Length Distribution for 2023 only ")
Lower_length_stats_2023_plot

print("Binned Length Distribution for 2024 only ")
Lower_length_stats_2024_plot


```
#### 2.1.4 Length-By-Weight Analysis

```{r}

#fit the LBV length-by weight relationship allometric model. 

#aL^b is the model, we estimate parameters for a and b. 

#this is the allometric weight-length model 

#one poorly recorded weight of zero is fixed with this line. 
wl.lin_lower <- lm_lw(data_full_Lower_n1n2%>%drop_na(Fork_length)%>%filter(Weight >0))


summary(wl.lin_lower)

# Remember, as a linear model the intercept is equal to ln(alpha) and the slope coefficient is equal to the beta parameter

#coef(wl.lin_lower)

# Finally lets save our parameter estimates
# Remember if we want the mean-unbiased estimate of the alpha parameter we need
#   to account for the lognormal correction when re-transforming

wl.lin.sigma_lower <- sigma(wl.lin_lower) 
print("sigma: ")
wl.lin.sigma_lower
# There is an analytical solution for the error standard deviation 

# ln(alpha)
wl.lin.ln_alpha_lower <- as.numeric(coef(wl.lin_lower)[1])

# alpha (mean-unbiased)# this is from the fish 622 course. 
wl.lin.alpha_lower <-exp(wl.lin.ln_alpha_lower)*exp((wl.lin.sigma_lower^2)/2)
print("alpha:")
wl.lin.alpha_lower

# Beta
wl.lin.beta_lower <- as.numeric(coef(wl.lin_lower)[2])
print("beta: ")
wl.lin.beta_lower

print("Lets plot the model fit")
plot_wl(data=data_full_Lower_n1n2, alpha=wl.lin.alpha_lower, beta=wl.lin.beta_lower)

```

It looks like the beta value for Lower tangle lake is a bit higher than shallow round tangle lake, but still hovering around 3.4/3.5. 


#### 2.1.5 What if we used the length-corrected data? Length composition analysis with length-corrected data for Lower tangle lake

```{r, echo = FALSE}



data_full_Lower_trunc_copy<- data_full_Lower_truncated

#do the assignment 
data_full_Lower_trunc_copy$Fork_length<-data_full_Lower_trunc_copy$corrected_length

#now we can proceed with the same analyses as above. 

#first we will do some data wrangling and visualization 

data_full_Lower_n1n2_corrected_trunc<- data_full_Lower_trunc_copy%>%
  filter(grouping %in% c("n1", "n2"))

data_full_Lower_n1n2_corrected_trunc<- data_full_Lower_n1n2_corrected_trunc %>%
  mutate(Year_capped = year(Date))

Lower_continuous_viz_trunc<- plot_length_dist_continuous(data_full_Lower_n1n2_corrected_trunc, lakeSystem = "Lower")

#it only makes sense to look at both years now because the 2024 data has been corrected, so no longer really represents 2024.

#if we look at both years overlayed, we can see that with the correction the 2024 histogram has a maximum density at a much lower fork length. 

print("Let's visualize the non-binned, length corrected continuous length distribution in a few ways")

Lower_continuous_viz_trunc$full_distribution_bothyears

Lower_continuous_viz_trunc$full_distribution

Lower_continuous_viz_trunc$full_distribution_years_sep

#bin the lengths
data_full_Lower_n1n2_trunc_binned<- assign_bins(data_full_Lower_n1n2_corrected_trunc)


Lower_length_stats_bothyears_trunc<- binned_length_statistics(data_full_Lower_n1n2_trunc_binned)

Lower_length_stats_2023_corrected_trunc<- binned_length_statistics(data_full_Lower_n1n2_trunc_binned, yearSort = TRUE, year = 2023)

Lower_length_stats_2024_corrected_trunc<- binned_length_statistics(data_full_Lower_n1n2_trunc_binned, yearSort = TRUE, year = 2024)


#make some plots for both years, 2023 and 2024

Lower_length_stats_bothyears_plot_corrected_trunc<- plot_length_binnedWithError(Lower_length_stats_bothyears_trunc)

Lower_length_stats_2023_plot_corrected_trunc<- plot_length_binnedWithError(Lower_length_stats_2023_corrected_trunc)

Lower_length_stats_2024_plot_corrected_trunc<- plot_length_binnedWithError(Lower_length_stats_2024_corrected_trunc)


print("Binned CORRECTED Length Distribution for 2023/2024 combined ")
Lower_length_stats_bothyears_plot_corrected_trunc

print("Binned CORRECTED Length Distribution for 2023 only ")
Lower_length_stats_2023_plot_corrected_trunc

print("Binned CORRECTED Length Distribution for 2024 only ")
Lower_length_stats_2024_plot_corrected_trunc


```


## 2.2 Estimating Population Size

#### 2.2.1 Consistency Tests for the Peterson Estimator

```{r, echo = FALSE}
#this spatial dataset is useful for 
sf_df_nounk <- sf_df %>%
  filter(Floy != "UNK" | is.na(Floy))




#need to go by stats now. 
sf_df_Lower_r1<- st_crop(sf_df_nounk, lower_lake_region_1)
sf_df_Lower_r1$region<-1
sf_df_Lower_r2<- st_crop(sf_df_nounk, lower_lake_region_2)
sf_df_Lower_r2$region<-2
sf_df_Lower_r3<- st_crop(sf_df_nounk, lower_lake_region_3)
sf_df_Lower_r3$region<-3

sf_df_Lower_regions<- sf_df_Lower_r1%>%
  rbind(sf_df_Lower_r2)%>%
  rbind(sf_df_Lower_r3)

### TEST FOR EQUAL PROBABILITY OF CAPTURE IN EVENT 1 AND EVENT 2 and chi2 mark recap test. 


##################################
###MATT TYERS RECAPR CODE FOR CONSISTENCY STUFF
##################################


#total sample sizes by stratum: 
#remember, we separated m2 and n2, so need to add for matt's code. 

sf_df_Lower_regions_nosmall<- sf_df_Lower_regions%>%filter(corrected_length >= min_size_capture_Lower$min)
counts_by_strata_Lower <- sf_df_Lower_regions_nosmall %>% group_by(region) %>% 
  summarize(n1 = sum(stat == "n1"), n2 = sum(stat == "n2")+sum(stat == "m2"), m2 = sum(stat == "m2"))

matrix_Lower<- petersen_matrix_tyerscode(sf_df_Lower_regions)
matrix_Lower$"NA"<-NULL

consistencytest(counts_by_strata_Lower$n1, counts_by_strata_Lower$n2, stratamat = matrix_Lower)



```
As we can see above, the first test didn't work because of columns with total zeros, but the second two tests did not reject the null, and therefore we can assume that the test is consistent. 

#### 2.2.2 Population Estimation

We will do a population estimation. We use the truncated dataset to correct for 2024 fish that were possible to catch in 2023. We will perform this estimate both including the two fish that moved from lower to shallow, and not including them. 

```{r, echo = FALSE}



#we apply the filters. 

data_full_Lower_truncated_pop<- data_full_Lower_truncated%>%
  filter(stat == grouping)%>%
  filter(cap_label %in% c("n1", "n2", "m2"))


Lower_lake_data_nomovers<- data_full_Lower_truncated_pop%>%
  filter(!Floy %in% c("19970", "18608"))

#now we do the pop estimates. First with the two floys included. 

petersen_estimate_trunc_Lower<- pop_calc(petersen_funcs,  data_full_Lower_truncated_pop)
Chapman_estimate_trunc_Lower<- pop_calc(Chapman_funcs, data_full_Lower_truncated_pop)
bailey_estimate_trunc_Lower<- pop_calc(Bailey_funcs, data_full_Lower_truncated_pop)

#Now we will do this with the two floys that moved not included. 

petersen_estimate_trunc_Lower_nomove<- pop_calc(petersen_funcs,  Lower_lake_data_nomovers)
Chapman_estimate_trunc_Lower_nomove<- pop_calc(Chapman_funcs, Lower_lake_data_nomovers)
bailey_estimate_trunc_Lower_nomove<- pop_calc(Bailey_funcs, Lower_lake_data_nomovers)

print("petersen estimate including the two floys that moved: ")
petersen_estimate_trunc_Lower

print("petersen estimate not including the two floys that moved: ")
petersen_estimate_trunc_Lower_nomove
```

The population estimate is lower when the two fish that moved are not included because they were n1 in lower and n2 in shallow/round, meaning they were recapped in shallowround. becayse pop = n1n2/m2, decreasing n1 by leaving these fish out decreases the value of the numerator. 

## 3.0.0 Finally we will look at Upper tangle lakes (Part 3)

Note: the Upper tangle lakes analysis has very little data and therefore does not meet the precision requirements outlined in the operational plan. 

### 3.1 Evaluating Growth and Length Composition Analysis 

#### 3.1.1 Visualization of ECDF with and Without Length-correction for 2024 (N2) Fish

```{r, echo = FALSE}
Upper_lake_data<- total_data%>%
  filter(lake_combined == "Upper")

Upper_lake_data_floys<-Upper_lake_data%>%
  select(Floy)%>%
  filter(!is.na(Floy))

floys_upper<-df_long%>%
  filter(Floy %in% Upper_lake_data_floys$Floy)


plotly::ggplotly(
  ggplot(floys_upper, aes(x = Date, y = length, group = Floy, color = factor(Floy))) +
    geom_point()+
    geom_line() +
    labs(title = "Fish Length over time Shallow Lake", x = "Date", y = "Length (cm)") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    scale_color_discrete(name = "Fish ID") +
    theme_bw()+
    theme(legend.position = "none")
) 

ggsave("fish_growth_upper.png", 
       height = 12,  # Adjust the height (in inches)
       width = 6,    # Adjust the width (in inches)
       dpi = 300)



min_size_capture_Upper<- Upper_lake_data%>%
  filter(Date < "2024-01-01")%>%
  filter(!is.na(Fork_length))%>%
  summarize(
    min = min(as.numeric(Fork_length))
  )


data_full_Upper<- data_full %>%
  filter(lake_combined == "Upper")%>%
  filter(cap_label %in% events_list)

#first let's look at the ecdf without truncating the distribution. 
ecdf_plot_withcorrection_Upper_full_regression<-ggplot(data_full_Upper, aes(x = as.numeric(Fork_length), color = grouping)) +
  stat_ecdf(geom = "step") +  # ECDF with step lines
  labs(x = "length", y = "CDF", color = "Group", title = "m2-length corrected ECDF Upper using full regression, no min-truncation") +
  theme_minimal()

print("first let's look at the ecdf without truncating the distribution or correcting for length-changes") 

ecdf_plot_withcorrection_Upper_full_regression


ggsave("ecdf_plot_withcorrection_Upper_full_regression.png", ecdf_plot_withcorrection_shallowround_full_regression, dpi = 300)

#Now let's truncate to the minimum captured size in event 1, which as we said above is min_size_capture_shallowround


#we keep fewer allowable fish using the full regression than just the shallow-round regression
data_full_Upper_truncated<- data_full_Upper%>%
  filter(corrected_length>= min_size_capture_Upper$min)



ecdf_plot_withcorrection_Upper_full_regression_truncated<-ggplot(data_full_Upper_truncated, aes(x = as.numeric(corrected_length), color = grouping)) +
  stat_ecdf(geom = "step") +  # ECDF with step lines
  labs(x = "length", y = "CDF", color = "Group", title = "m2-length corrected ECDF Shallow-Round using full regression, with min-truncation") +
  theme_minimal()

print("Now let's truncate to the minimum captured size in event 1, and with corrected lengths for n2")

ecdf_plot_withcorrection_Upper_full_regression_truncated

ggsave("ecdf_plot_withcorrection_Upper_full_regression_truncated.png", ecdf_plot_withcorrection_Upper_full_regression_truncated, dpi = 300)



```
As we can see, there are not many datapoints in the upper tangle lakes, but correcting for length does not really change the length distribution, likely because upper tangle lakes already has many big fish. 

#### 3.1.2 K-S tests with and without length-correction

```{r, echo = FALSE}
Upper_ks_nocorrection<- ks_test(data_full_Upper, events_list, "Upper", length_variable = "Fork_length" )

Upper_ks_withcorrection<- ks_test(data_full_Upper_truncated, events_list, "Upper", length_variable = "corrected_length")

print("KS-test results without length correction:")
Upper_ks_nocorrection

print("KS test results with length correction: ")
Upper_ks_withcorrection
```
We do not see any evidence for stratification, but we must take this with a grain of salt because we have very small sample sizes and low power. 

#### 3.1.3 Length Composition Analysis Without growth correction 


```{r, echo = FALSE}

#when making these distributions, we want to make sure that we are only considering n1 and n2, and not using the m2 data twice. 

data_full_Upper_n1n2<- data_full_Upper%>%
  filter(grouping %in% c("n1", "n2"))

data_full_Upper_n1n2 <- data_full_Upper_n1n2 %>%
  mutate(Year_capped = year(Date))


#now we can run some of the functions from the Length_distribution file. 


#just visualizing the continuous distribution first 

Upper_continuous_viz<- plot_length_dist_continuous(data_full_Upper_n1n2, lakeSystem = "Upper")


print("Let's visualize the non-binned continuous length distribution in a few ways")

Upper_continuous_viz$full_distribution
Upper_continuous_viz$full_distribution_bothyears
Upper_continuous_viz$full_distribution_years_sep

#bin the lengths

data_full_Upper_n1n2<- data_full_Upper_n1n2%>%
  drop_na(Fork_length)%>%
  filter(Fork_length!= "?")

data_full_Upper_n1n2_binned<- assign_bins(data_full_Upper_n1n2)

#do an analysis for both years, 2023 and 2024

Upper_length_stats_bothyears<- binned_length_statistics(data_full_Upper_n1n2_binned)


print("Now we have binned the Upper lake data, let's take a look at the stats for both years")
knitr::kable(Upper_length_stats_bothyears, caption = "Binned Length Stats Upper 2023 + 2024")

#the preservation needs to happen here. 
Upper_length_stats_2023<- binned_length_statistics(data_full_Upper_n1n2_binned, yearSort = TRUE, year = 2023)

print("Let's look at just 2023 for Upper lake")
knitr::kable(Upper_length_stats_2023, caption = "Binned Length Stats Upper 2023")

Upper_length_stats_2024<- binned_length_statistics(data_full_Upper_n1n2_binned, yearSort = TRUE, year = 2024)

print("Let's look at just 2024 for Upper lake")
knitr::kable(Upper_length_stats_2024, caption = "Binned Length Stats Upper 2024")

#make some plots for both years, 2023 and 2024

Upper_length_stats_bothyears_plot<- plot_length_binnedWithError(Upper_length_stats_bothyears)


Upper_length_stats_2023_plot<- plot_length_binnedWithError(Upper_length_stats_2023)

Upper_length_stats_2024_plot<- plot_length_binnedWithError(Upper_length_stats_2024)

print("Binned Length Distribution for 2023/2024 combined ")
Upper_length_stats_bothyears_plot

print("Binned Length Distribution for 2023 only ")
Upper_length_stats_2023_plot

print("Binned Length Distribution for 2024 only ")
Upper_length_stats_2024_plot


```

#### 3.1.4 Length-By-Weight Analysis

```{r}

#fit the LBV length-by weight relationship allometric model. 

#aL^b is the model, we estimate parameters for a and b. 

#this is the allometric weight-length model 

#one poorly recorded weight of zero is fixed with this line. 
wl.lin_Upper <- lm_lw(data_full_Upper_n1n2%>%drop_na(Fork_length)%>%filter(Weight >0))


summary(wl.lin_Upper)

# Remember, as a linear model the intercept is equal to ln(alpha) and the slope coefficient is equal to the beta parameter

#coef(wl.lin_Upper)

# Finally lets save our parameter estimates
# Remember if we want the mean-unbiased estimate of the alpha parameter we need
#   to account for the lognormal correction when re-transforming

wl.lin.sigma_Upper <- sigma(wl.lin_Upper) 
print("sigma: ")
wl.lin.sigma_Upper
# There is an analytical solution for the error standard deviation 

# ln(alpha)
wl.lin.ln_alpha_Upper <- as.numeric(coef(wl.lin_Upper)[1])

# alpha (mean-unbiased)# this is from the fish 622 course. 
wl.lin.alpha_Upper <-exp(wl.lin.ln_alpha_Upper)*exp((wl.lin.sigma_Upper^2)/2)
print("alpha:")
wl.lin.alpha_Upper

# Beta
wl.lin.beta_Upper <- as.numeric(coef(wl.lin_Upper)[2])
print("beta: ")
wl.lin.beta_Upper

print("Lets plot the model fit")
plot_wl(data=data_full_Upper_n1n2, alpha=wl.lin.alpha_Upper, beta=wl.lin.beta_Upper)

```

Here, the beta value is very close to shallow-round tangle lakes and a bit strewed from lower tangle lakes. 

#### 3.1.5 What if we used the length-corrected data? Length composition analysis with length-corrected data for Upper tangle lake

```{r, echo = FALSE}



data_full_Upper_trunc_copy<- data_full_Upper_truncated

#do the assignment 
data_full_Upper_trunc_copy$Fork_length<-data_full_Upper_trunc_copy$corrected_length

#now we can proceed with the same analyses as above. 

#first we will do some data wrangling and visualization 

data_full_Upper_n1n2_corrected_trunc<- data_full_Upper_trunc_copy%>%
  filter(grouping %in% c("n1", "n2"))

data_full_Upper_n1n2_corrected_trunc<- data_full_Upper_n1n2_corrected_trunc %>%
  mutate(Year_capped = year(Date))

Upper_continuous_viz_trunc<- plot_length_dist_continuous(data_full_Upper_n1n2_corrected_trunc, lakeSystem = "Upper")

#it only makes sense to look at both years now because the 2024 data has been corrected, so no longer really represents 2024.

#if we look at both years overlayed, we can see that with the correction the 2024 histogram has a maximum density at a much Upper fork length. 

print("Let's visualize the non-binned, length corrected continuous length distribution in a few ways")

Upper_continuous_viz_trunc$full_distribution_bothyears

Upper_continuous_viz_trunc$full_distribution

Upper_continuous_viz_trunc$full_distribution_years_sep

#bin the lengths
data_full_Upper_n1n2_trunc_binned<- assign_bins(data_full_Upper_n1n2_corrected_trunc)


Upper_length_stats_bothyears_trunc<- binned_length_statistics(data_full_Upper_n1n2_trunc_binned)

Upper_length_stats_2023_corrected_trunc<- binned_length_statistics(data_full_Upper_n1n2_trunc_binned, yearSort = TRUE, year = 2023)

Upper_length_stats_2024_corrected_trunc<- binned_length_statistics(data_full_Upper_n1n2_trunc_binned, yearSort = TRUE, year = 2024)


#make some plots for both years, 2023 and 2024

Upper_length_stats_bothyears_plot_corrected_trunc<- plot_length_binnedWithError(Upper_length_stats_bothyears_trunc)

Upper_length_stats_2023_plot_corrected_trunc<- plot_length_binnedWithError(Upper_length_stats_2023_corrected_trunc)

Upper_length_stats_2024_plot_corrected_trunc<- plot_length_binnedWithError(Upper_length_stats_2024_corrected_trunc)


print("Binned CORRECTED Length Distribution for 2023/2024 combined ")
Upper_length_stats_bothyears_plot_corrected_trunc

print("Binned CORRECTED Length Distribution for 2023 only ")
Upper_length_stats_2023_plot_corrected_trunc

print("Binned CORRECTED Length Distribution for 2024 only ")
Upper_length_stats_2024_plot_corrected_trunc


```
Because the sample size estimates are not met for both 2023 and 2024 for precision criterion, it may make sense to use corrected lengths and only estimate for 2023 catch. 

## 3.2 Estimating Population Size

#### 3.2.1 Consistency Tests for the Peterson Estimator

```{r, echo = FALSE}
#this spatial dataset is useful for 
sf_df_nounk <- sf_df %>%
  filter(Floy != "UNK" | is.na(Floy))




#need to go by stats now. 
sf_df_Upper_r1<- st_crop(sf_df_nounk, upper_lake_region_1)
sf_df_Upper_r1$region<-1
sf_df_Upper_r2<- st_crop(sf_df_nounk, upper_lake_region_2)
sf_df_Upper_r2$region<-2
sf_df_Upper_r3<- st_crop(sf_df_nounk, upper_lake_region_3)
sf_df_Upper_r3$region<-3

sf_df_Upper_regions<- sf_df_Upper_r1%>%
  rbind(sf_df_Upper_r2)%>%
  rbind(sf_df_Upper_r3)

### TEST FOR EQUAL PROBABILITY OF CAPTURE IN EVENT 1 AND EVENT 2 and chi2 mark recap test. 


##################################
###MATT TYERS RECAPR CODE FOR CONSISTENCY STUFF
##################################


#total sample sizes by stratum: 
#remember, we separated m2 and n2, so need to add for matt's code. 

sf_df_Upper_regions_nosmall<- sf_df_Upper_regions%>%filter(corrected_length >= min_size_capture_Upper$min)
counts_by_strata_Upper <- sf_df_Upper_regions_nosmall %>% group_by(region) %>% 
  summarize(n1 = sum(stat == "n1"), n2 = sum(stat == "n2")+sum(stat == "m2"), m2 = sum(stat == "m2"))

matrix_Upper<- petersen_matrix_tyerscode(sf_df_Upper_regions)
matrix_Upper$"NA"<-NULL

consistencytest(counts_by_strata_Upper$n1, counts_by_strata_Upper$n2, stratamat = matrix_Upper)

```
There are again very small sample sizes, but we can continue with the Petersen estimate as two consistency tests were passed

```{r, echo = FALSE}

#we apply the filters. 

data_full_Upper_truncated_pop<- data_full_Upper_truncated%>%
  filter(stat == grouping)%>%
  filter(cap_label %in% c("n1", "n2", "m2"))



#now we do the pop estimates. First with the two floys included. 

petersen_estimate_trunc_Upper<- pop_calc(petersen_funcs,  data_full_Upper_truncated_pop)
Chapman_estimate_trunc_Upper<- pop_calc(Chapman_funcs, data_full_Upper_truncated_pop)
bailey_estimate_trunc_Upper<- pop_calc(Bailey_funcs, data_full_Upper_truncated_pop)


print("petersen estimate: ")
petersen_estimate_trunc_Upper

```
As we can see we have a very small population estimate for upper tangle lake, with very low power. 

